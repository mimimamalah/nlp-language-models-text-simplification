{"cells":[{"cell_type":"markdown","metadata":{"id":"4j05XaIOUvcf"},"source":["#  Assignment 1 - Language Model Foundations"]},{"cell_type":"markdown","metadata":{"id":"GA3bGKeGUvcl"},"source":["<div style=\"padding:15px 20px 20px 20px;border-left:3px solid green;background-color:#e4fae4;border-radius: 20px;color:#424242;\">\n","\n","Welcome to the **1st assignment** for the **CS-552: Modern NLP course**!\n","\n","- üòÄ Name: **< First Last >**\n","- ‚úâÔ∏è Email: **< first.last >@epfl.ch**\n","- ü™™ SCIPER: **XXXXXX**\n","\n","In the first two parts of this assignment, you need to construct a dataset and use it to train language models (LSTM and Transformer);\n","\n","In the third part, you will finetune language models (RNN-based and Transformer-based Encoder-Decoder) on a text simplification task.\n","\n","### **Tasks**\n","- **[PART 1: Data Preprocessing](#1)**\n","    - [1.1 Data Cleaning](#11)\n","    - [1.2 Build Vocabulary](#12)\n","    - [1.2 Get PyTorch Dataset](#13)\n","- **[PART 2: Training Language Models](#2)**\n","    - [2.1 Vanilla LSTM](#21)\n","    - [2.2 Transformer (DistilGPT2)](#22)\n","- **[PART 3: Finetuning Language Models](#3)**\n","    - [3.1 Encoder-Decoder Model](#31)\n","    - [3.2 Transformer (T5)](#32)\n","\n","\n","### **Deliverables**\n","- ‚úÖ This Jupyter notebook\n","- ‚úÖ `data.py`, `modeling.py` file\n","- ‚úÖ Checkpoints for two LSTM-variant and DistilGPT2 language models (Part 2)\n","- ‚úÖ Checkpoints for finetuned encoder-decoder and T5 language models (Part 3)\n","- ‚úÖ `./tensorboard` directory with logs for all trained/finetuned models\n","\n","Large files such as model checkpoints and logs should be pushed to the repository with Git LFS. You may also find that training the models on a GPU can speed up the process, we recommend using Colab's free GPU service for this. A tutorial on how to use Git LFS and Colab can be found [here](https://github.com/epfl-nlp/cs-552-modern-nlp/blob/main/Exercises/tutorials.md).\n","\n","</div>"]},{"cell_type":"markdown","metadata":{},"source":["<div style=\"padding:15px 20px 20px 20px;border-left:3px solid orange;background-color:#fff5d6;border-radius: 20px;color:#424242;\">\n","\n","## How to implement this assignment\n","\n","Please read carefully the following points. All the information on how to read, implement and submit your assignment is explained in detail below.\n","\n","1. For this assignment, you will need to implement and fill in the missing code snippets for both the **Jupyter Notebook** `assignment1.ipynb` and the `data.py` and `modeling.py` python files. In the `data.py` and `modeling.py` files, you will add all the Dataset and Model classes you will implement according to the skeleton present in the files. In the notebook, you will use the definitions from those python files to prepare datasets, train and finetune models and add the report at the end.\n","\n","2. To implement your coding part, you can import the external libraries we provide in the `requirements.txt` file, however, you should not use any other package not included in these requirements. We recommend using **python=3.10** for this assignment\n","\n","3. At the end of the notebook, you will need to fill in a **report** template, providing the results of your implementation. We provide you with the template for the report, therefore you need to fill in the missing Markdown cells with the requested information. \n","\n","4. Along with the `assignment1.ipynb` and the `data.py`, `modeling.py` files, you need to additionally upload model files under the `models/` dir, regarding the following models:\n","    - the two LSTM-variant models (PART 2)  \n","    - the trained-from-scratch DistilGPT2 model (PART 2) \n","    - the fine-tuned Encoder-Decoder model (PART 3) \n","    - the fine-tuned T5 model (PART 3)\n","    \n","You will provide test results on all of the model variants according to the report template.\n","\n","5. Finally, you will need to log your training pipelines using Tensorboard. Please follow the instructions in the `README.md` of the [tensorboard/](tensorboard/README.md) directory.\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# This cell makes sure modules are auto-loaded when you change external python files\n","%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# If you are working in Colab, then consider mounting your assignment folder to your drive\n","# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# and change the path below to point to the assignment folder\n","# %cd /content/drive/MyDrive/path-to-your-assignment-folder"]},{"cell_type":"markdown","metadata":{"id":"YYTL0zJ1nIE_"},"source":["### Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":205,"status":"ok","timestamp":1707662501644,"user":{"displayName":"Simin Fan","userId":"15154949338491351516"},"user_tz":-60},"id":"eiPc1rRNiS__","outputId":"bf7a2474-666b-495e-a833-4322826231cd"},"outputs":[],"source":["import os\n","os.environ[\"WANDB_DISABLED\"] = \"true\"\n","os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" # limiting to one GPU"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uexGOS0GiS__"},"outputs":[],"source":["!pip install -r requirements.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q0bmJE-0iTAA"},"outputs":[],"source":["from datasets import load_dataset\n","import math\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.optim as optim\n","import torch.nn as nn\n","import evaluate\n","import gensim\n","import transformers\n","import nltk"]},{"cell_type":"markdown","metadata":{},"source":["<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;color:#424242;\">\n","\n","**TODOüîª: Enter your SCIPER number below!**\n","     \n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import re\n","import random\n","import numpy as np\n","import torch\n","\n","## Enter your SCIPER here: ##\n","SCIPER = \"\"\n","\n","try:\n","    assert re.match(\"\\d{6}\", SCIPER)[0] == SCIPER, \"Invalid SCIPER given. please enter your correct 6-digit SCIPER number above!\"\n","except:\n","    print(\"Invalid SCIPER given. please enter your correct 6-digit SCIPER number above!\")\n","\n","student_seed = int(SCIPER)\n","\n","\n","\"\"\"Set seed for reproducibility.\"\"\"\n","random.seed(student_seed)\n","np.random.seed(student_seed)\n","torch.manual_seed(student_seed)\n","torch.cuda.manual_seed_all(student_seed)"]},{"cell_type":"markdown","metadata":{"id":"m06WLo6qUvcm"},"source":["---\n","\n","<a name=\"1\"></a>\n","# PART 1: Data Preprocessing\n","\n","We will train a language model using the `wikitext-103` dataset.\n","\n","> `wikitext-103` dataset is a large collection of articles with verified\n","good and featured quality from Wikipedia. We will use the open-source dataset from Huggingface (https://huggingface.co/datasets/wikitext)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v4hUSpJMzuM9"},"outputs":[],"source":["from utils import *\n","from test_A1 import *\n","from data import filter_by_length, data_clean, count_tokens, build_vocabulary"]},{"cell_type":"markdown","metadata":{"id":"jBU8CTeo1Uy4"},"source":["### Load `wikitext-103` Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2486,"status":"ok","timestamp":1707662515805,"user":{"displayName":"Simin Fan","userId":"15154949338491351516"},"user_tz":-60},"id":"F_ImpO7n05kj","outputId":"ba208190-5450-40e1-daf5-69dcea4067cc"},"outputs":[],"source":["wikitext_dataset = load_dataset(\"wikitext\", 'wikitext-103-v1', split=\"train\")\n","\n","print(f\"Size of the dataset is {len(wikitext_dataset)}\")\n","\n","# You can print some samples to get a sense of the data\n","print(wikitext_dataset[0])\n","print(wikitext_dataset[10])\n","print(wikitext_dataset[100])"]},{"cell_type":"markdown","metadata":{"id":"ZOD9aSZIiTAC"},"source":["<a id=\"11\"></a>\n","## üéØ Q1.1: **Data Cleaning**.\n","\n","We find some problems in the dataset:\n","- some samples are empty (with no tokens) or too short;\n","- some contains noisy texts (e.g. `==== <> ====`);\n","- some have non-English texts.\n","\n","So we need a data cleaning before feeding it to the model.\n","\n","***Test:*** After each function, you can test your implementation by running a simple test case. (Note: passing these simple cases do not necessarily mean your implementation is 100% correct.)"]},{"cell_type":"markdown","metadata":{"id":"YfMJBW7OAnj-"},"source":["<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;color:#424242;\">\n","\n","**We will perform 4-step data cleaning:**\n","\n","- **Step 1**: filter out sequences shorter than `min_len=100` or longer than `max_len=128`;\n","- **Step 2**: filter out sequences with particular pattern `= * =\\n`, where `*` denotes any possible sequences;\n","- **Step 3**: filter out Non-English sequences;\n","- **Step 4**: lowercase all sequences.\n","\n","**TODOüîª: Implement `filter_by_length` and `data_clean` function in `data.py`**.\n","\n","More instructions are provided in the function description.\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fXz9W7tZiTAD"},"outputs":[],"source":["# ETS: take ~1-3mins to run #\n","from data import data_clean\n","\n","wikitext_dataset = data_clean(wikitext_dataset, min_len=100, max_len=128)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1707662518086,"user":{"displayName":"Simin Fan","userId":"15154949338491351516"},"user_tz":-60},"id":"AHPGnLRn5qKK","outputId":"e57eab88-6755-402c-feb5-fb63cba2d349"},"outputs":[],"source":["test_data_clean(wikitext_dataset)"]},{"cell_type":"markdown","metadata":{"id":"6Mmm46nr6SZK"},"source":["<a id=\"12\"></a>\n","## üéØ Q1.2: **Build Vocabulary**.\n","\n","\n","<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;color:#424242;\">\n","\n","**We have to build a vocabulary dictionary for the cleaned `wikitext_dataset` following four steps:**\n","\n","- **Step 1**: Compute frequency of occurences for each unique token in the dataset;\n","\n","- **Step 2**: Find a set of rare tokens with frequency lower than `min_freq=5`. Replace them with `unk_token='<unk>'`;\n","\n","- **Step 3**: Filter out sequences with more than 15\\% rare tokens. (*We implemented this for you.*)\n","\n","- **Step 4**: Recompute the token frequency to get final vocabulary dict.\n","\n","**TODOüîª: Implement `count_tokens` and `build_vocabulary` function in `data.py`**.\n","\n","More instructions are provided in the function description.\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":40385,"status":"ok","timestamp":1707662560874,"user":{"displayName":"Simin Fan","userId":"15154949338491351516"},"user_tz":-60},"id":"6pywsRBT7YMu","outputId":"ad48f120-0628-499b-e49c-22a23b8ecb46"},"outputs":[],"source":["# ETS: take ~1-3mins to run #\n","from data import build_vocabulary\n","\n","wikitext_dataset, token_freq_dict = build_vocabulary(wikitext_dataset, min_freq=5, unk_token='<unk>')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1707662560875,"user":{"displayName":"Simin Fan","userId":"15154949338491351516"},"user_tz":-60},"id":"sljyM8tt8h6n","outputId":"0cd45532-22b0-4765-86fb-3610dbc7a9d4"},"outputs":[],"source":["test_vocab_build(wikitext_dataset, token_freq_dict)"]},{"cell_type":"markdown","metadata":{"id":"drmgakSq97Hm"},"source":["**Let's look at the vocabulary distribution with a histogram:**\n","\n","- we can observe that tokens with very low frequency are filtered out :)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":434},"executionInfo":{"elapsed":1047,"status":"ok","timestamp":1707662385158,"user":{"displayName":"Simin Fan","userId":"15154949338491351516"},"user_tz":-60},"id":"Lgzc0VT6-GrM","outputId":"62caff42-ae6c-4c73-8f16-025e88d0076c"},"outputs":[],"source":["plt.hist(token_freq_dict.values(), bins=10**np.linspace(0,6,33))\n","plt.xscale(\"log\")"]},{"cell_type":"markdown","metadata":{"id":"07Q5by1kiTAQ"},"source":["<a id=\"13\"></a>\n","## üéØ Q1.3: **Build Pytorch Dataset**.\n","\n","<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;color:#424242;\">\n","\n","**We build a Pytorch Dataset using cleaned `wikitext_dataset`:**\n","\n","- **Step 1**: we have to add some special tokens for each sequence:\n","  - `<start>`: add to the start of a sequence;\n","  - `<stop>`: add to the end of a sequence;\n","  - `<pad>`: padding token. add padding token until the length of the sequence reach `MAX_SEQ_LENGTH`.\n","\n","- **Step 2**: when we fetch a sequence in the dataset by indexing, it should return a sequence of token indices.\n","\n","**TODOüîª: Implement `__init__` and `__getitem__` function in `RNNDataset` class in `data.py`**.\n","\n","Detailed instructions are provided in the function description.\n","</div>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31793,"status":"ok","timestamp":1707662592665,"user":{"displayName":"Simin Fan","userId":"15154949338491351516"},"user_tz":-60},"id":"SmSdlO_BBbFO","outputId":"ec0880c3-3a54-46ab-c024-36d3d567fdd4"},"outputs":[],"source":["from data import RNNDataset\n","\n","# ETS: take ~1min to run #\n","MAX_SEQ_LENGTH = 128\n","rnn_dataset = RNNDataset(dataset=wikitext_dataset,\n","                         max_seq_length=MAX_SEQ_LENGTH)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":211,"status":"ok","timestamp":1707662592866,"user":{"displayName":"Simin Fan","userId":"15154949338491351516"},"user_tz":-60},"id":"i3HrGoeNC7ur","outputId":"d03c14ff-9d39-49be-a00f-322cc7f24bb2"},"outputs":[],"source":["test_rnn_dataset(rnn_dataset)"]},{"cell_type":"markdown","metadata":{"id":"KvfS1HLxE7NP"},"source":["<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;color:#424242;\">\n","\n","**Once we create the `rnn_dataset`, we have to split it into train/test dataset, and build a DataLoader for each::**\n","\n","- **Step 1**: split `rnn_dataset` into train/test datasets, with `test_ratio=0.1`:\n","\n","- **Step 2**: Build pytorch dataloader for train/test dataset with `batch_size=8`.\n","\n","**TODOüîª: Implement `get_dataloader` function in `data.py`**.\n","\n","Detailed instructions are provided in the function description.\n","</div>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f8TP_CXoE5V6"},"outputs":[],"source":["from data import get_dataloader\n","\n","# ETS: ~1min #\n","train_dataloader, test_dataloader = get_dataloader(rnn_dataset, test_ratio=0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1707662592866,"user":{"displayName":"Simin Fan","userId":"15154949338491351516"},"user_tz":-60},"id":"0PGShHPNHevY","outputId":"461f9160-d854-4efa-c334-137f06c63829"},"outputs":[],"source":["test_dataloaders(train_dataloader, test_dataloader)"]},{"cell_type":"markdown","metadata":{"id":"8ZpVeuyWiTAU"},"source":["<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #8e7cc3;background-color:#e4e1eb;border-radius: 15px;color:#424242;\">\n","\n","üéâ **Excellent work!** By this point, we have all core building blocks for Part 2 :)\n","   \n","- [X] `rnn_dataset`: A `Dataset` obj with the data, the vocabulary, the pad index, and mapping from tokens to indices;\n","- [X] `train_dataloader`: A `DataLoader` obj with training sequences;\n","- [X] `test_dataloader`: A `DataLoader` obj with testing sequences.\n","\n","_Tip: Try to familiarize with functionalities and attributes they provide._\n","    \n","</div>"]},{"cell_type":"markdown","metadata":{"id":"HPDNtdxFUvcn"},"source":["---\n","\n","<a id=\"2\"></a>\n","# PART 2:  Training Language Models\n","\n","#### Language Model: a probabilistic model of a sequence of tokens.\n","\n","üîµ **What?**\n","\n","Language modeling (LM) is the use of various statistical and probabilistic techniques to determine the probability of a given sequence of words occurring in a sentence. Language models analyze bodies of text data to provide a basis for their word predictions. They are used in natural language processing (NLP) applications, particularly ones that generate text as an output. Some of these applications include, machine translation and question-answering.\n","\n","üü° **How?**\n","\n","There are several different probabilistic approaches to modeling language, which vary depending on the purpose of the language model. From a technical perspective, the various types differ by the amount of text data they analyze and the math they use to analyze it (architecture). Some LMs we've already seen and will learn about during lectures are n-gram / count-based models, Recurrent Neural Networks (RNNs), and Transformer models.\n","\n","üü£ **Why?**\n","\n","Language modeling is crucial in modern NLP applications. It is the reason that machines can understand qualitative information. Each language model type, in one way or another, turns qualitative information into quantitative information. This allows people to communicate with machines as they do with each other to a limited extent. It is used directly in a variety of industries including tech, finance, healthcare, transportation, legal, military and government. Additionally, it's likely most people reading this have interacted with a language model in some way at some point in the day, whether it be through Google search, an autocomplete text function or engaging with a voice assistant.\n","\n","‚ÑπÔ∏è Source: [Original article](https://www.techtarget.com/searchenterpriseai/definition/language-modeling#:~:text=Language%20models%20determine%20word%20probability,predict%20or%20produce%20new%20sentences.)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"X_CK0Zl7iTAU"},"source":["<div style=\"padding:15px 15px 15px 15px;border-left:3px solid gray;background-color:#F3F3F3;border-radius: 15px;color:#424242;\">\n","\n","**In this part, you will train your own language models using the dataset created in Part 1.**\n","\n","You need to implement **3 different model variants**;\n","train and evaluate their perplexity.\n","    \n","| Model | Variant | Description |\n","|:---- |:----- | :----- |\n","|**LSTM** | Token embeddings trained from scratch | An LSTM model with a trainable token Embedding layer <br>that will be initialized randomly and trained from scratch along with the LM. |\n","| | Pre-trained token embeddings| An LSTM model with pre-trained GloVe embeddings as input <br>that will be ***frozen*** while the LM is training. |\n","| **Transformer** | Trained from scratch | A Transformer based model that follows the architecture of [DistilGPT2](https://huggingface.co/distilgpt2). |\n","    \n","</div>"]},{"cell_type":"markdown","metadata":{"id":"Smi1xBm3iTAV"},"source":["<a id=\"21\"></a>\n","## üéØ Q2.1: **Vanilla LSTM**.\n","\n","\n","<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;color:#424242;\">\n","\n","**First, we train a vanilla LSTM on our train/test dataLoaders:**\n","\n","- **Step 1**: Build the `VanillaLSTM` class, which takes:\n","  - `vocab_size`: size of vocabulary,\n","  - `embedding_dim`: dimension of token embedding,\n","  - `hidden_dim`: hidden dimension of lstm layer;\n","  - `num_layers`: number of stacked lstm layers;\n","  - `embedding_weights`: pretrained embedding weights. Train from scratch if set to `None`. Default: None;\n","  - `freeze_embeddings`: whether to freeze pretrained embeddings. Only be functional when `embedding_weights` is not None.\n","\n","- **Step 2**: Train the LSTM model with `train_dataloader`. We usually run only one epoch for language modelling task to prevent overfitting;\n","\n","- **Step 3**: Evaluate the LSTM by perplexity scores on `test_dataloader`.\n","\n","**TODOüîª: Implement `VanillaLSTM`, `train_lstm` and `test_lstm` function in `modeling.py`**.\n","\n","Detailed instructions are provided in the function description.\n","</div>\n"]},{"cell_type":"markdown","metadata":{"id":"Wsyfaq8yUg0j"},"source":["#### **LSTM from Scratch**\n","\n","We here train the LSTM embedding weights from scratch. The dimension of embedding is defined as `embedding_dim`.\n","\n","We set the `embedding_dim` as 100 here to compare with gensim pretrained embeddings."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pIdJ9c9-Uvco"},"outputs":[],"source":["from modeling import VanillaLSTM\n","\n","# Define the parameters to VanillaLSTM\n","## Hint: what do we have from Part1?\n","vocab_size = len(rnn_dataset.token2idx)\n","embedding_dim = 100\n","hidden_dim = 100\n","num_layers = 2\n","dropout_rate = 0.15\n","lr = 1e-3\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PcfLRY96S2uk"},"outputs":[],"source":["model = VanillaLSTM(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate).to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1707662593184,"user":{"displayName":"Simin Fan","userId":"15154949338491351516"},"user_tz":-60},"id":"eJ2Pqr0wTwAv","outputId":"cf6f2087-2188-4526-844d-6f67624ffe03"},"outputs":[],"source":["test_lstm_scratch(model)"]},{"cell_type":"markdown","metadata":{"id":"BbpZkBndX259"},"source":["##### Train&Evaluation (LSTM-scratch)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yZTG7WHHTLd6"},"outputs":[],"source":["# Define optimizer and criterion (loss function)\n","## Hint: what do we have from Part1?\n","optimizer = optim.Adam(model.parameters(), lr=lr)\n","criterion = nn.CrossEntropyLoss(ignore_index=rnn_dataset.pad_idx)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5rHIOncbT1sW"},"outputs":[],"source":["from modeling import train_lstm, test_lstm\n","\n","# ETS: take ~30mins for training #\n","train_lstm(model, train_dataloader, optimizer, criterion, device=device, tensorboard_path=\"./tensorboard/lstm_scratch\")\n","torch.save(model.state_dict(), 'models/lstm_scratch.pt')\n","test_lstm(model, test_dataloader, criterion, device=device)"]},{"cell_type":"markdown","metadata":{"id":"3wBTcLdQUotA"},"source":["#### **LSTM with Pretrained Embeddings**\n","\n","We first download the gensim embeddings, then fix the embedding layer in the LSTM model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qnQp3iCNUrmB"},"outputs":[],"source":["import gensim.downloader\n","# Download the \"glove-wiki-gigaword-100\" embeddings\n","glove_vectors = gensim.downloader.load('glove-wiki-gigaword-100')\n","initial_embedding_weight = torch.nn.Embedding(len(rnn_dataset.token2idx),\n","                                              embedding_dim).weight.detach().numpy()\n","\n","# Get the pretrained embeddings from gensim for each tokens\n","for word_type, idx in rnn_dataset.token2idx.items():\n","  if word_type in glove_vectors.key_to_index.keys():\n","      initial_embedding_weight[idx] = glove_vectors[word_type]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"etXoX2HDVrzj"},"outputs":[],"source":["# Define the parameters to VanillaLSTM\n","## Hint: Use the same params as LSTM_scratch\n","vocab_size = len(rnn_dataset.token2idx)\n","embedding_dim = 100\n","hidden_dim = 100\n","num_layers = 2\n","dropout_rate = 0.15\n","lr = 1e-3\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EBUZJet7XbpS"},"outputs":[],"source":["model = VanillaLSTM(vocab_size, embedding_dim, hidden_dim,\n","                    num_layers, dropout_rate=dropout_rate,\n","                    embedding_weights=initial_embedding_weight,\n","                    freeze_embeddings=True).to(device)\n","\n","## Define optimizer and criterion\n","optimizer = optim.Adam(model.parameters(), lr=lr)\n","criterion = nn.CrossEntropyLoss(ignore_index=rnn_dataset.pad_idx)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1707662620832,"user":{"displayName":"Simin Fan","userId":"15154949338491351516"},"user_tz":-60},"id":"AtlqPsZeZg7s","outputId":"59b9101a-c92c-416e-eb81-6c09ddfdec07"},"outputs":[],"source":["test_lstm_pretrained(model)"]},{"cell_type":"markdown","metadata":{"id":"ZyRP0QzGX8_4"},"source":["##### Train&Evaluation (LSTM-pretrained)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RZN6B8L3XxSa"},"outputs":[],"source":["# ETS: take ~30mins for training #\n","train_lstm(model, train_dataloader, optimizer, criterion, device=device, tensorboard_path=\"./tensorboard/lstm_pretrained\")\n","torch.save(model.state_dict(), 'models/lstm_pretrained.pt')\n","test_lstm(model, test_dataloader, criterion, device=device)"]},{"cell_type":"markdown","metadata":{"id":"vFYGXHj1YESh"},"source":["#### **Compare LSTM-scratch and LSTM-pretrained?**\n","\n","**TODOüîª: compare the *performance* and *training efficiency* between `LSTM-scratch` and `LSTM-pretrained`**."]},{"cell_type":"markdown","metadata":{"id":"UGW7lXk7YIvI"},"source":["\n","\n","ans: [Enter your findings here. ]"]},{"cell_type":"markdown","metadata":{"id":"ieaNrQIJzow-"},"source":["<a id=\"22\"></a>\n","## üéØ Q2.2: **Train a Transformer: DistillGPT2**.\n","\n","\n","<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;color:#424242;\">\n","\n","**We train a DistillGPT2 on our train/test dataLoaders:**\n","\n","- **Step 1**: Rebuild the datasets and dataloaders using huggingface tokenizer;\n","\n","- **Step 2**: Train a DistilGPT2 efficiently using Trainer form `transformers`.\n","\n","- **Step 3**: Evaluate perplexity scores on `test_dataloader`.\n","\n","**TODOüîª: follow the instructions below**.\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jUWqgNzvzoj-"},"outputs":[],"source":["from transformers import AutoConfig, AutoModelWithLMHead, AutoTokenizer, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n","\n","model_name = \"distilgpt2\"\n","tokenizer_checkpoint = \"distilgpt2\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2675,"status":"ok","timestamp":1707660065541,"user":{"displayName":"Simin Fan","userId":"15154949338491351516"},"user_tz":-60},"id":"QLaG6iasiTAd","outputId":"8568c927-1698-4e29-fb0c-c32e860ef5d5"},"outputs":[],"source":["MAX_SEQ_LENGTH = 128\n","\n","# Load model from scratch\n","model_config = AutoConfig.from_pretrained(model_name)\n","gpt2_scratch_model = AutoModelWithLMHead.from_config(model_config)\n","gpt_tokenizer = AutoTokenizer.from_pretrained(model_name)\n","gpt_tokenizer.pad_token = gpt_tokenizer.eos_token"]},{"cell_type":"markdown","metadata":{"id":"Z29EEczseE9-"},"source":["#### **Tokenize Datasets**\n"," Huggingface provides pretrained tokenizer for `DistilGPT2`, so we have to tokenize our dataset with this tokenizer.\n","\n","**TODOüîª: Implement `get_encoded_dataset`**."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":145,"referenced_widgets":["d9487cb740524a9989e94ce5d87e8c98","c7c1bcb88c7a4f48ba5f818a4a697f5e","fd072dd48e394637ae6b4068e4ad30de","c22beae6fb16496e9f3f25239341becf","138871ee82ff47c3af673a08bd3ec4d0","093cb168c2b24cfbb199171a0f60b96e","6263c708fb1448de8587a657fcf2d38f","827526a107f745b0b686efe5da122833","5da3fb9adbaa4d99a44b2054506c8b12","8b6fd1a431a14aea98b821f4aaf34a31","70f25545be2d42ccad3d36fd1974f942","bb2190eebc7e428b96d4bede0f1f9d55","192d348aadd647b081f0f52a232fc953","c88866f880d74e83a45d94c624f2a30e","7540eab42af64d088cbb5d3e92c994a5","d66f1974b6e34111b538985e810f3837","f9155aa243924a1c8135ca6d06a63d1b","68953a0dfe4345238ceb3c41e647dbc7","61616b00cc534fa8b321a5dbad789e24","7095989fcb954bbbb0512bb5363f19c4","ff2575473f3443c1b71d30f29b507cca","1da2811f157d44b9bf33518ddaaf7442","9d87dc786c0c4b7a92140a6859c86e2f","8250f82beb194339b7a0272dc7da6bbe","04d7f784399e421493ffbd8d02e007da","f56202dca8854edfb0fdc69c131cad6b","1dfcf13e5feb491aa0fd84f03851f326","de7a729fa16544de9b9480e41f1a9d60","eb23d79b77ca4e5e920032339b4eabb8","285a4d5837b2481f9a9c94902d501f31","c38330ee41c94fe08dc11c047fb57db8","98297cde5bff45c594db633c7845e8a1","a1d3e6439dad4c57b8f94115a153fe25","33a9fa0d8d604710bec9fc806a6917d2","830aa101c887417a918712f2a5ee18ef","3f3b978021f34c93941567c169f312b4","1dbcea3369004cc8bcbb563845e8185b","eb11abc069224b2f9e8ee35f403f9b88","3bf734883fb64e8b97625db34c364f40","0fb98d2d737d4148a75386484c838fb8","2a5278ebb1774d2b83a4dbeb47a2cd9b","da19f2f9eaeb4724aae004722a1b1e3e","c308ad3e35d64a428afa303d9bdd3fde","d191da7183e440838dc22930b47eb76e"]},"executionInfo":{"elapsed":563338,"status":"ok","timestamp":1707660628873,"user":{"displayName":"Simin Fan","userId":"15154949338491351516"},"user_tz":-60},"id":"6CrcpJNQceve","outputId":"5b634f1a-baca-4286-ed76-7999d44c53e8"},"outputs":[],"source":["def get_encoded_dataset(wikitext_dataset):\n","  # TODO: Tokenize wikitext_dataset with DistilGPT2 tokenizer. Padding to `max_length`.\n","  encoded_dataset = ...\n","  # TODO: Filter out samples longer than MAX_SEQ_LENGTH=128\n","  encoded_dataset = ...\n","\n","  encoded_dataset = encoded_dataset.map(\n","    lambda example: {\"input_ids\": example['input_ids'][:MAX_SEQ_LENGTH],\n","                     \"attention_mask\": example[\"attention_mask\"][:MAX_SEQ_LENGTH]})\n","  encoded_dataset = encoded_dataset.remove_columns(\"text\")\n","  encoded_dataset = encoded_dataset.with_format(\"torch\")\n","  encoded_dataset = encoded_dataset.map(lambda example:{\"labels\": example[\"input_ids\"]})\n","  return encoded_dataset\n","\n","## ETS: ~10mins to run.\n","encoded_dataset = get_encoded_dataset(wikitext_dataset)\n","transformer_train_dataset, transformer_test_dataset = torch.utils.data.random_split(encoded_dataset,\n","                                                               [int(len(encoded_dataset)*0.9), len(encoded_dataset)-int(len(encoded_dataset)*0.9)])"]},{"cell_type":"markdown","metadata":{"id":"DdNMjJm4gK8W"},"source":["#### **Build Trainer**\n"," Huggingface provides very efficient `Trainer` wrapper for language model training and evaluation.\n","\n","**TODOüîª: Run the following blocks to define the training arguements and trainer, then start training simply by `trainer.train()`**."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":528,"status":"ok","timestamp":1707661298931,"user":{"displayName":"Simin Fan","userId":"15154949338491351516"},"user_tz":-60},"id":"JV7Zq35AiTAf","outputId":"aaea89f4-f2a3-4151-dbe3-ad78beda8f67"},"outputs":[],"source":["data_collator = DataCollatorForLanguageModeling(tokenizer=gpt_tokenizer, mlm=False)\n","training_args = TrainingArguments(\n","    output_dir=f\"{model_name}-wikitext103\",\n","    evaluation_strategy = \"steps\",\n","    num_train_epochs = 1.0,\n","    logging_steps=500,\n","    learning_rate=2e-5,\n","    save_steps=1000,\n","    weight_decay=0.01,\n","    report_to=\"tensorboard\",\n","    logging_dir=\"./tensorboard/distilgpt2-scratch\")\n","\n","trainer = Trainer(\n","    model=gpt2_scratch_model,\n","    args=training_args,\n","    tokenizer=gpt_tokenizer,\n","    train_dataset=transformer_train_dataset,\n","    eval_dataset=transformer_test_dataset,\n","    data_collator=data_collator,)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":435},"executionInfo":{"elapsed":26184,"status":"error","timestamp":1707661333228,"user":{"displayName":"Simin Fan","userId":"15154949338491351516"},"user_tz":-60},"id":"W9gVEQ6nUvcp","outputId":"8bef41ee-c8b3-4fa8-fa21-c2f509e31e9b","scrolled":true},"outputs":[],"source":["# ETS: ~20mins to run.\n","trainer.train()\n","trainer.save_model('models/distilgpt2_scratch')"]},{"cell_type":"markdown","metadata":{"id":"zXHew822jUZl"},"source":["**TODOüîª: Run evaluation by `trainer.evaluate()`**."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":124},"executionInfo":{"elapsed":37833,"status":"ok","timestamp":1707661376315,"user":{"displayName":"Simin Fan","userId":"15154949338491351516"},"user_tz":-60},"id":"HiuwHEY6iTAg","outputId":"d548a5df-50c0-4349-d393-4899666c2117","scrolled":true},"outputs":[],"source":["# ETS: ~1min to run.\n","eval_result = trainer.evaluate()\n","perplexity_from_scratch = math.exp(eval_result[\"eval_loss\"])\n","print(f\"The perplexity on the test dataset is {perplexity_from_scratch:.3f}\")"]},{"cell_type":"markdown","metadata":{"id":"zyg5BbR6jgoB"},"source":["#### **Compare to the Pretrained DistilGPT2**\n","\n","**TODOüîª: Load the pretrained `DistilGPT2` checkpoint from Huggingface.\n","We only run evaluation of the `trainsformer_test_dataset` here. Compare the performance between `gpt2_scratch_model` and `gpt2_pretrained_model`.**."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OJpB8U-1iTAh"},"outputs":[],"source":["from transformers import AutoModelForCausalLM, AutoTokenizer\n","from transformers import Trainer, TrainingArguments\n","\n","model_id = \"distilgpt2\"\n","gpt2_pretrained_model = AutoModelForCausalLM.from_pretrained(model_id)\n","tokenizer_pretrained_gpt = AutoTokenizer.from_pretrained(model_id)\n","tokenizer_pretrained_gpt.pad_token = tokenizer_pretrained_gpt.eos_token\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":197,"status":"ok","timestamp":1707661642031,"user":{"displayName":"Simin Fan","userId":"15154949338491351516"},"user_tz":-60},"id":"bHrdXr64iTAh","outputId":"668daeec-67ea-4677-bfa1-095c952b1caf"},"outputs":[],"source":["training_args = TrainingArguments(\n","    output_dir=f\"pretrained_{model_id}-wikitext103\",\n","    evaluation_strategy = \"steps\",\n","    num_train_epochs = 1.0,\n","    logging_steps=500,\n","    learning_rate=2e-5,\n","    save_steps=1000,\n","    weight_decay=0.01,\n","    report_to=\"tensorboard\",\n","    logging_dir=\"./tensorboard/distilgpt2-pretrained\")\n","\n","pretrained_trainer = Trainer(\n","    model=gpt2_pretrained_model,\n","    args=training_args,\n","    tokenizer=tokenizer_pretrained_gpt,\n","    train_dataset=transformer_train_dataset,\n","    eval_dataset=transformer_test_dataset,\n","    data_collator=data_collator,)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"elapsed":38088,"status":"ok","timestamp":1707661693096,"user":{"displayName":"Simin Fan","userId":"15154949338491351516"},"user_tz":-60},"id":"Rm_7YZY6iTAi","outputId":"a2127f6b-fec2-4534-839b-7bfaa035e948"},"outputs":[],"source":["# ETS: ~1min.\n","eval_result = pretrained_trainer.evaluate()\n","perplexity_pretrained_model = math.exp(eval_result[\"eval_loss\"])\n","print(f\"The perplexity of pretrained {model_id} on the test dataset is {perplexity_pretrained_model:.3f}\")"]},{"cell_type":"markdown","metadata":{"id":"6FxdACC5iTAi"},"source":["<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #8e7cc3;background-color:#e4e1eb;border-radius: 15px;color:#424242;\">\n","    \n","üéâ  **Excellent work!** We have completed Langauge Modelling tasks!\n","\n","#### Part 2 - Checklist\n","Here are the core building blocks you created and that you will need for Part 3:\n","   \n","- [X] LSTM-variants checkpoints.\n","- [X] LSTM-variants ppl scores.\n","- [X] Transformer-variants ppl scores.\n","\n","</div>"]},{"cell_type":"markdown","metadata":{},"source":["Just before you move on to the Part 3, let's save a few things which we will use in the Part 3 so that you don't have to run Part 1 and 2 every time."]},{"cell_type":"markdown","metadata":{},"source":["#### Save wikitext vocabulary"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from utils import save_binary\n","\n","save_binary(rnn_dataset.dataset_vocab, \"wikitext_vocab.pkl\")"]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","<a id=\"3\"></a>\n","# PART 3: Fine-tune on the Text Simplification task üöÄ\n","\n","In this part we will fine-tune and test language models on the downstream task of text simplification (a.k.a text compression). In this task, given a piece of text, the goal is to simplify (or compress) this text by extracting the most important information from it. We will consider encoder-decoder models built on top of the LSTM models you built in Part 2 and also pretrained transformer models from Huggingface.\n"," \n","Hence, there will be two sections in this final part of the assignment:\n","- [3.1 Finetune a custom Encoder-Decoder model on text simplification task](#32)\n","- [3.2 Finetune pretrained T5 model (an encoder-decoder) model on text simplification task](#33)\n","\n","### Dataset\n","In this part, we will use a sentence compression [dataset](https://huggingface.co/datasets/embedding-data/sentence-compression) available on Huggingface. Each row in this dataset contains two sentences: the original sentence and the simplified one.\n","\n","An example from this dataset is the following:\n","\n","**Original sentence:** \"The JSE kept toying with an all time high by midday today as resources continued to fuel the bourse.\"\n","\n","**Simplified sentence:** \"JSE keeps toying with all time high\"\n","\n","This dataset contains around 180k examples, but we will only use a subset of it."]},{"cell_type":"markdown","metadata":{},"source":["### Load and preprocess sentence-compression dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# We will use NLTK to tokenize the text\n","from datasets import load_dataset\n","import nltk\n","nltk.download('punkt')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["scomp = load_dataset(\"embedding-data/sentence-compression\")"]},{"cell_type":"markdown","metadata":{},"source":["Let's inspect the dataset. Note that this dataset only contains a train split (we will create our own train, validation and test sets from this split later)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["scomp[\"train\"][0]"]},{"cell_type":"markdown","metadata":{},"source":["First to make our dataset and sentences smaller, let's filter examples where the original sentence has more than 10 and less than 100 words."]},{"cell_type":"markdown","metadata":{},"source":["<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;color:#424242;\">\n","\n","üéØ Goal: Keep only the examples that has more than 10 and less than 100 words in its original sentence.\n","\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["scomp_small = ..."]},{"cell_type":"markdown","metadata":{},"source":["Next, let's define our train, validation and test sets of size 10k, 1k, and 1k respectively and also get rid of the 'set' attribute."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["scomp_train, scomp_val, scomp_test = scomp_small[\"train\"][:10000]['set'], scomp_small[\"train\"][10000:11000]['set'], scomp_small[\"train\"][11000:12000]['set']"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"22\"></a>\n","## üéØ Q3.1: **Finetune custom encoder-decoder model on sentence simplification task**.\n","\n","\n","<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px; color:#424242;\">\n","\n","**In this section, we will build a model based on the [encoder-decoder architecture](https://arxiv.org/abs/1409.3215) and finetune it on our text simplification task. As you probably know by now that vanilla encoder-decoder models do not perform well when we have long-range dependencies. Therefore, we will build one where the decoder uses [additive attention](https://arxiv.org/abs/1409.0473) over encoder outputs. You will need to finish the following main steps:**\n","\n","- **Step 1**: Implement a Tokenizer\n","\n","- **Step 2**: Build a Pytorch Dataset\n","\n","- **Step 3**: Implement an Encoder-Decoder Model\n","\n","- **Step 4**: Finetune the model on the task\n","\n","- **Step 5**: Test and evaluate model performance\n","\n","**TODOüîª: follow the instructions below**.\n","</div>"]},{"cell_type":"markdown","metadata":{},"source":["### Implement a tokenizer\n","We will first implement a tokenizer for our dataset. This tokenizer will take our vocabulary from the Part 1 and will have methods to encode a given text and decode a given text ids."]},{"cell_type":"markdown","metadata":{},"source":["<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;color:#424242;\">\n","\n","üéØ Goal:  **Go to `data.py` and implement the `CustomTokenizer` class.**\n","    \n","</div>"]},{"cell_type":"markdown","metadata":{},"source":["Let's first load our wikitext vocabulary from Part 1."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from utils import load_binary\n","\n","wikitext_vocab = load_binary(\"wikitext_vocab.pkl\")"]},{"cell_type":"markdown","metadata":{},"source":["Initialize the custom tokenizer with this vocabulary."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from data import CustomTokenizer\n","\n","custom_tokenizer = CustomTokenizer(vocab=wikitext_vocab)"]},{"cell_type":"markdown","metadata":{},"source":["### Implement a pytorch dataset\n","Now we will implement a Pytorch dataset that will represent our data in a suitable format for our model fine-tuning later."]},{"cell_type":"markdown","metadata":{},"source":["<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;color:#424242;\">\n","\n","üéØ Goal:  **Go to `data.py` and implement the `SCompDataset` class.**\n","    \n","</div>"]},{"cell_type":"markdown","metadata":{},"source":["Initialize train, validation and test pytorch datasets."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from data import SCompDataset\n","\n","MAX_SEQ_LENGTH = 128\n","scomp_train_ds = SCompDataset(scomp_train, custom_tokenizer, MAX_SEQ_LENGTH)\n","scomp_val_ds = SCompDataset(scomp_val, custom_tokenizer, MAX_SEQ_LENGTH)\n","scomp_test_ds = SCompDataset(scomp_test, custom_tokenizer, MAX_SEQ_LENGTH)"]},{"cell_type":"markdown","metadata":{},"source":["Initialize dataloaders for our datasets respectively."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torch.utils.data import DataLoader\n","\n","# feel free to change batch size according to your GPU memory\n","scomp_train_dataloader = DataLoader(scomp_train_ds, batch_size=32, shuffle=True)\n","scomp_val_dataloader = DataLoader(scomp_val_ds, batch_size=32, shuffle=True)\n","scomp_test_dataloader = DataLoader(scomp_test_ds, batch_size=32, shuffle=True)"]},{"cell_type":"markdown","metadata":{},"source":["### Implement an Encoder-Decoder model\n","In this section, you will implement a model based on Encoder-Decoder architecture where the encoder will be one of the LSTM variants you implemented in Part 2. You will specifically implement an attention-based encoder-decoder where decoder attends over the encoder outputs to allow it to handle long range dependencies well."]},{"cell_type":"markdown","metadata":{},"source":["<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;color:#424242;\">\n","\n","üéØ Goal:  **Go to `modeling.py` and implement the `Encoder`, `AdditiveAttention`, `Decoder` and `EncoderDecoder` classes.**\n","    \n","</div>"]},{"cell_type":"markdown","metadata":{},"source":["Next we will load a pretrained encoder from Part 2. Choose the encoder that had best performance based on the perplexity metric."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from modeling import VanillaLSTM\n","\n","vocab_size = len(wikitext_vocab)\n","embedding_dim = 100\n","hidden_dim = 100\n","num_layers = 2\n","dropout_rate = 0.15\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","pretrained_encoder = VanillaLSTM(vocab_size, embedding_dim, hidden_dim,\n","                                  num_layers, dropout_rate=dropout_rate).to(device)\n","\n","# TODO: Load the pretrained model from the file\n","pretrained_encoder.load_state_dict(...)"]},{"cell_type":"markdown","metadata":{},"source":["Next we will need to implement training, evaluation and generation methods."]},{"cell_type":"markdown","metadata":{},"source":["<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;color:#424242;\">\n","\n","üéØ Goal:  **Go to `modeling.py` and implement the `seq2seq_train`, `seq2seq_eval` and `seq2seq_generate` functions.**\n","    \n","</div>"]},{"cell_type":"markdown","metadata":{},"source":["Now we are ready to finetune our encoder-decoder models on the sentence simplification dataset."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from modeling import EncoderDecoder\n","\n","lr = ...\n","dropout_rate = ...\n","bos_token_id = custom_tokenizer.bos_token_id\n","encoder_decoder = EncoderDecoder(hidden_dim, vocab_size, vocab_size, bos_token_id=bos_token_id, dropout_rate=dropout_rate, pretrained_encoder=pretrained_encoder).to(device)\n","optimizer = optim.Adam(encoder_decoder.parameters(), lr=lr)\n","criterion = nn.NLLLoss(ignore_index=custom_tokenizer.pad_token_id)\n","num_params = sum(p.numel() for p in encoder_decoder.parameters() if p.requires_grad)\n","print(f'The model has {num_params:,} trainable parameters')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from modeling import seq2seq_train\n","\n","# ETS: ~30 mins to run with a batch size of 32 for 20 epochs\n","seq2seq_train(model=encoder_decoder,\n","              train_loader=scomp_train_dataloader,\n","              eval_loader=scomp_val_dataloader,\n","              optimizer=optimizer,\n","              criterion=criterion,\n","              device=device,\n","              tensorboard_path=\"./tensorboard/encoder_decoder\")\n","# save the model\n","torch.save(encoder_decoder.state_dict(), \"models/encoder_decoder.pt\")"]},{"cell_type":"markdown","metadata":{},"source":["Now we have the finetuned encoder-decoder model, we will use it to generate simplified text from the test set and compute a rouge score."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from modeling import seq2seq_generate\n","test_generations = seq2seq_generate(encoder_decoder, scomp_test_dataloader, custom_tokenizer, device=device)"]},{"cell_type":"markdown","metadata":{},"source":["Inspect the generations. Do they look like reasonable outputs?"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_generations[:5]"]},{"cell_type":"markdown","metadata":{},"source":["Let's implement an evaluation function for ROUGE score.\n","\n","**ROUGE score** stands for Recall-Oriented Understudy for Gisting Evaluation. In its simplest form ROUGE score is the quotient of the matching words under the total count of words in reference sentence. Regarding the denominator ROUGE is a recall oriented metric. \n","\n","![rouge.png](docs/rouge.png)\n","\n","**ROUGE-L score** is based on the length of the longest common subsequence (LCS). To counter the disadvantages of a pure recall metric as in ROUGE-N, Rouge-L calculates the weighted harmonic mean (or f-measure) combining the precision score and the recall score.\n","\n","![rouge_l.png](docs/rouge_l.png)\n","\n","‚ÑπÔ∏è Source: [Original article](https://clementbm.github.io/theory/2021/12/23/rouge-bleu-scores.html#bleu)"]},{"cell_type":"markdown","metadata":{},"source":["<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;color:#424242;\">\n","\n","üéØ Goal:  **Go to `modeling.py` and implement the `evaluate_rouge` method.**\n","    \n","</div>"]},{"cell_type":"markdown","metadata":{},"source":["Calculate Rouge scores."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from modeling import evaluate_rouge\n","\n","print(evaluate_rouge(test_generations))"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"22\"></a>\n","## üéØ Q3.2: **Finetune pretrained T5 model on text simplification task**.\n","\n","\n","<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;color:#424242;\">\n","\n","**In this section, we will take a pretrained [T5](https://arxiv.org/abs/1910.10683) model which is also an encoder-decoder model but based on the transformer architecture and finetune it on our task. You will need to do the following main steps:**\n","\n","- **Step 1**: Load T5 Model from Huggingface\n","\n","- **Step 2**: Implement Pytorch dataset for this model\n","\n","- **Step 3**: Run model finetuning\n","\n","- **Step 4**: Test and evaluate model performance\n","\n","\n","**TODOüîª: follow the instructions below**.\n","</div>"]},{"cell_type":"markdown","metadata":{},"source":["Initialize T5 model from Huggingface pretrained model."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","\n","model_name = \"t5-small\"\n","t5_tokenizer = ...\n","# Make sure to put your model on the GPU\n","t5_model = ..."]},{"cell_type":"markdown","metadata":{},"source":["Let's define a pytorch dataset for our T5 model."]},{"cell_type":"markdown","metadata":{},"source":["<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;color:#424242;\">\n","\n","üéØ Goal:  **Go to `modeling.py` and implement the `SCompT5Dataset` class.**\n","    \n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from data import ScompT5Dataset\n","scomp_t5_train_dataset = ScompT5Dataset(scomp_train, t5_tokenizer)\n","scomp_t5_val_dataset = ScompT5Dataset(scomp_val, t5_tokenizer)\n","scomp_t5_test_dataset = ScompT5Dataset(scomp_test, t5_tokenizer)"]},{"cell_type":"markdown","metadata":{},"source":["Prepare finetuning arguments"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from transformers import TrainingArguments, Trainer\n","\n","# create the finetuning trainer\n","# TODO: set the hyperparameters to reasonable values\n","training_args = TrainingArguments(\n","    output_dir=f\"finetune_{model_name}-SCOMP\",\n","    evaluation_strategy=\"epoch\",\n","    per_device_train_batch_size=...,\n","    per_device_eval_batch_size=...,\n","    logging_steps=100,\n","    learning_rate=...,\n","    num_train_epochs=...,\n","    save_strategy=\"no\",\n","    weight_decay=...,\n","    report_to=\"tensorboard\",\n","    logging_dir=\"./tensorboard/t5-scomp\")"]},{"cell_type":"markdown","metadata":{},"source":["#### Run the model finetuning"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["trainer = Trainer(\n","    model=t5_model,\n","    args=training_args,\n","    train_dataset=scomp_t5_train_dataset,\n","    eval_dataset=scomp_t5_val_dataset\n",")\n","\n","trainer.train()\n","trainer.save_model(f\"models/finetune_{model_name}_scomp\")"]},{"cell_type":"markdown","metadata":{},"source":["Now we will load our final finetuned model from the last checkpoint and evaluate it on the test set."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from transformers import T5ForConditionalGeneration\n","t5_model = T5ForConditionalGeneration.from_pretrained(f\"models/finetune_{model_name}_scomp\").to(device)"]},{"cell_type":"markdown","metadata":{},"source":["<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;color:#424242;\">\n","\n","üéØ Goal:  **Go to `modeling.py` and implement the `t5_generate` method.**\n","    \n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from modeling import t5_generate\n","\n","test_t5_generations = t5_generate(scomp_t5_test_dataset, t5_model, t5_tokenizer)"]},{"cell_type":"markdown","metadata":{},"source":["Let's inspect the generations. Are they better than the ones generated by the encoder-decoder model? Provide some reasons why one model's generations are better than the other."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_t5_generations[:5]"]},{"cell_type":"markdown","metadata":{},"source":["**Answer:**"]},{"cell_type":"markdown","metadata":{},"source":["Finally, let's compute the ROUGE scores."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from modeling import evaluate_rouge\n","\n","print(evaluate_rouge(test_t5_generations))"]},{"cell_type":"markdown","metadata":{},"source":["<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #8e7cc3;background-color:#e4e1eb;border-radius: 15px;color:#424242;\">\n","\n","üéâ Excellent work! You just finished the code implementation parts of the assignment. \n","\n","#### Part 3 - Checklist\n","Here are the elements you will need for the report in Part 4:\n","\n","- [X] Finetuned encoder-decoder model and its rouge scores.\n","- [X] Finetuned T5 model and its rouge scores.\n","</div>"]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","<a id=\"4\"></a>\n","# PART 4: Write your report üìò\n","\n","Fill in the tables with the respective scores. "]},{"cell_type":"markdown","metadata":{},"source":["<div style=\"padding:15px 15px 15px 15px;border-left:3px solid #03befc;background-color:#eff7fe;border-radius: 15px;text-align:center;color:#424242;\">\n","\n","#### Perplexity results on Language Models\n","\n","| Model - Variant | PPL |\n","|:--------- | :-----: |\n","| LSTM Variant A - Embeddings trained from scratch | YOUR SCORE HERE |\n","| LSTM Variant B - Pre-trained embeddings & frozen | YOUR SCORE HERE |\n","||||\n","| DistilGPT2 - Trained from scratch | YOUR SCORE HERE |\n","| Pre-trained DistilGPT2 | YOUR SCORE HERE |\n","    \n","#### Performance scores on Text Simplification\n","| Model - Variant | ROUGE-1 | ROUGE-2 | ROUGE-L | ROUGE-Lsum |\n","|:--------- | :-----: | :-----: |  :-----: |  :-----: | \n","| Finetuned Encoder-Decoder| YOUR SCORE HERE |YOUR SCORE HERE |  YOUR SCORE HERE |YOUR SCORE HERE |\n","| Finetuned T5 | YOUR SCORE HERE |YOUR SCORE HERE |  YOUR SCORE HERE |YOUR SCORE HERE |\n","\n","</div>"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["YYTL0zJ1nIE_","m06WLo6qUvcm","jBU8CTeo1Uy4","ZOD9aSZIiTAC","6Mmm46nr6SZK","07Q5by1kiTAQ","HPDNtdxFUvcn","Smi1xBm3iTAV","Wsyfaq8yUg0j","BbpZkBndX259","3wBTcLdQUotA","ZyRP0QzGX8_4","ieaNrQIJzow-","Z29EEczseE9-","DdNMjJm4gK8W","zyg5BbR6jgoB"],"gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3.10.8 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}},"widgets":{"application/vnd.jupyter.widget-state+json":{"04d7f784399e421493ffbd8d02e007da":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_285a4d5837b2481f9a9c94902d501f31","max":192680,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c38330ee41c94fe08dc11c047fb57db8","value":192680}},"093cb168c2b24cfbb199171a0f60b96e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0fb98d2d737d4148a75386484c838fb8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"138871ee82ff47c3af673a08bd3ec4d0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"192d348aadd647b081f0f52a232fc953":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f9155aa243924a1c8135ca6d06a63d1b","placeholder":"‚Äã","style":"IPY_MODEL_68953a0dfe4345238ceb3c41e647dbc7","value":"Filter: 100%"}},"1da2811f157d44b9bf33518ddaaf7442":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1dbcea3369004cc8bcbb563845e8185b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c308ad3e35d64a428afa303d9bdd3fde","placeholder":"‚Äã","style":"IPY_MODEL_d191da7183e440838dc22930b47eb76e","value":" 192680/192680 [00:51&lt;00:00, 3754.06 examples/s]"}},"1dfcf13e5feb491aa0fd84f03851f326":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"285a4d5837b2481f9a9c94902d501f31":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2a5278ebb1774d2b83a4dbeb47a2cd9b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"33a9fa0d8d604710bec9fc806a6917d2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_830aa101c887417a918712f2a5ee18ef","IPY_MODEL_3f3b978021f34c93941567c169f312b4","IPY_MODEL_1dbcea3369004cc8bcbb563845e8185b"],"layout":"IPY_MODEL_eb11abc069224b2f9e8ee35f403f9b88"}},"3bf734883fb64e8b97625db34c364f40":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f3b978021f34c93941567c169f312b4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2a5278ebb1774d2b83a4dbeb47a2cd9b","max":192680,"min":0,"orientation":"horizontal","style":"IPY_MODEL_da19f2f9eaeb4724aae004722a1b1e3e","value":192680}},"5da3fb9adbaa4d99a44b2054506c8b12":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"61616b00cc534fa8b321a5dbad789e24":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6263c708fb1448de8587a657fcf2d38f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"68953a0dfe4345238ceb3c41e647dbc7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7095989fcb954bbbb0512bb5363f19c4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"70f25545be2d42ccad3d36fd1974f942":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7540eab42af64d088cbb5d3e92c994a5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ff2575473f3443c1b71d30f29b507cca","placeholder":"‚Äã","style":"IPY_MODEL_1da2811f157d44b9bf33518ddaaf7442","value":" 257472/257472 [03:29&lt;00:00, 1247.92 examples/s]"}},"8250f82beb194339b7a0272dc7da6bbe":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_de7a729fa16544de9b9480e41f1a9d60","placeholder":"‚Äã","style":"IPY_MODEL_eb23d79b77ca4e5e920032339b4eabb8","value":"Map: 100%"}},"827526a107f745b0b686efe5da122833":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"830aa101c887417a918712f2a5ee18ef":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3bf734883fb64e8b97625db34c364f40","placeholder":"‚Äã","style":"IPY_MODEL_0fb98d2d737d4148a75386484c838fb8","value":"Map: 100%"}},"8b6fd1a431a14aea98b821f4aaf34a31":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"98297cde5bff45c594db633c7845e8a1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d87dc786c0c4b7a92140a6859c86e2f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8250f82beb194339b7a0272dc7da6bbe","IPY_MODEL_04d7f784399e421493ffbd8d02e007da","IPY_MODEL_f56202dca8854edfb0fdc69c131cad6b"],"layout":"IPY_MODEL_1dfcf13e5feb491aa0fd84f03851f326"}},"a1d3e6439dad4c57b8f94115a153fe25":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bb2190eebc7e428b96d4bede0f1f9d55":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_192d348aadd647b081f0f52a232fc953","IPY_MODEL_c88866f880d74e83a45d94c624f2a30e","IPY_MODEL_7540eab42af64d088cbb5d3e92c994a5"],"layout":"IPY_MODEL_d66f1974b6e34111b538985e810f3837"}},"c22beae6fb16496e9f3f25239341becf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8b6fd1a431a14aea98b821f4aaf34a31","placeholder":"‚Äã","style":"IPY_MODEL_70f25545be2d42ccad3d36fd1974f942","value":" 257472/257472 [01:51&lt;00:00, 2103.77 examples/s]"}},"c308ad3e35d64a428afa303d9bdd3fde":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c38330ee41c94fe08dc11c047fb57db8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c7c1bcb88c7a4f48ba5f818a4a697f5e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_093cb168c2b24cfbb199171a0f60b96e","placeholder":"‚Äã","style":"IPY_MODEL_6263c708fb1448de8587a657fcf2d38f","value":"Map: 100%"}},"c88866f880d74e83a45d94c624f2a30e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_61616b00cc534fa8b321a5dbad789e24","max":257472,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7095989fcb954bbbb0512bb5363f19c4","value":257472}},"d191da7183e440838dc22930b47eb76e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d66f1974b6e34111b538985e810f3837":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d9487cb740524a9989e94ce5d87e8c98":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c7c1bcb88c7a4f48ba5f818a4a697f5e","IPY_MODEL_fd072dd48e394637ae6b4068e4ad30de","IPY_MODEL_c22beae6fb16496e9f3f25239341becf"],"layout":"IPY_MODEL_138871ee82ff47c3af673a08bd3ec4d0"}},"da19f2f9eaeb4724aae004722a1b1e3e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"de7a729fa16544de9b9480e41f1a9d60":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb11abc069224b2f9e8ee35f403f9b88":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb23d79b77ca4e5e920032339b4eabb8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f56202dca8854edfb0fdc69c131cad6b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_98297cde5bff45c594db633c7845e8a1","placeholder":"‚Äã","style":"IPY_MODEL_a1d3e6439dad4c57b8f94115a153fe25","value":" 192680/192680 [03:11&lt;00:00, 1016.46 examples/s]"}},"f9155aa243924a1c8135ca6d06a63d1b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fd072dd48e394637ae6b4068e4ad30de":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_827526a107f745b0b686efe5da122833","max":257472,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5da3fb9adbaa4d99a44b2054506c8b12","value":257472}},"ff2575473f3443c1b71d30f29b507cca":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}
